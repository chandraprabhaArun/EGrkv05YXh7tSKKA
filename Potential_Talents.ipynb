{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "KtmoqubVz2A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFBhIla_V7VU",
        "outputId": "3648200a-cf48-48ca-9644-49927513c401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/potential-talents - Aspiring human resources - seeking human resources.csv\")"
      ],
      "metadata": {
        "id": "Jdw0cOqjfmIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert \"500+\" to \"500\" and leave the rest unchanged\n",
        "def convert_connection(connection):\n",
        "    if '+' in str(connection):\n",
        "        return int(str(connection).replace('+', ''))\n",
        "    else:\n",
        "        return int(connection)"
      ],
      "metadata": {
        "id": "dIBxyrQfWEIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the conversion to the \"connection\" column\n",
        "df['connection'] = df['connection'].apply(convert_connection)"
      ],
      "metadata": {
        "id": "T47hNOZMWRui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the \"connection\" column to the range [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "df['scaled_connection'] = scaler.fit_transform(df[['connection']])\n"
      ],
      "metadata": {
        "id": "AKeYULhqWWmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "OaCOTdxxWamE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to each job title\n",
        "df['processed_job_title'] = df['job_title'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "WSCpfLzfWej2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize and tag documents\n",
        "def tag_documents(df):\n",
        "    tagged_data = []\n",
        "    for i, row in df.iterrows():\n",
        "        tagged_data.append(TaggedDocument(words=row['processed_job_title'], tags=[str(row.name)]))\n",
        "    return tagged_data"
      ],
      "metadata": {
        "id": "ZQIRlo_JWnab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and tag documents\n",
        "tagged_data = tag_documents(df)\n",
        "print(tagged_data)\n"
      ],
      "metadata": {
        "id": "vq1iJMQDWqqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541ce754-8875-42e4-f234-de5d08fc4831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['0']), TaggedDocument(words=['nativ', 'english', 'teacher', 'epik', 'english', 'program', 'korea'], tags=['1']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['2']), TaggedDocument(words=['peopl', 'develop', 'coordin', 'ryan'], tags=['3']), TaggedDocument(words=['advisori', 'board', 'member', 'celal', 'bayar', 'univers'], tags=['4']), TaggedDocument(words=['aspir', 'human', 'resourc', 'specialist'], tags=['5']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['6']), TaggedDocument(words=['hr', 'senior', 'specialist'], tags=['7']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['8']), TaggedDocument(words=['seek', 'human', 'resourc', 'hri', 'generalist', 'posit'], tags=['9']), TaggedDocument(words=['student', 'chapman', 'univers'], tags=['10']), TaggedDocument(words=['svp', 'chro', 'market', 'commun', 'csr', 'offic', 'engi', 'houston', 'woodland', 'energi', 'gphr', 'sphr'], tags=['11']), TaggedDocument(words=['human', 'resourc', 'coordin', 'intercontinent', 'buckhead', 'atlanta'], tags=['12']), TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['13']), TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['14']), TaggedDocument(words=['nativ', 'english', 'teacher', 'epik', 'english', 'program', 'korea'], tags=['15']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['16']), TaggedDocument(words=['peopl', 'develop', 'coordin', 'ryan'], tags=['17']), TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['18']), TaggedDocument(words=['nativ', 'english', 'teacher', 'epik', 'english', 'program', 'korea'], tags=['19']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['20']), TaggedDocument(words=['peopl', 'develop', 'coordin', 'ryan'], tags=['21']), TaggedDocument(words=['advisori', 'board', 'member', 'celal', 'bayar', 'univers'], tags=['22']), TaggedDocument(words=['aspir', 'human', 'resourc', 'specialist'], tags=['23']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['24']), TaggedDocument(words=['hr', 'senior', 'specialist'], tags=['25']), TaggedDocument(words=['aspir', 'human', 'resourc', 'manag', 'student', 'seek', 'internship'], tags=['26']), TaggedDocument(words=['seek', 'human', 'resourc', 'opportun'], tags=['27']), TaggedDocument(words=['aspir', 'human', 'resourc', 'manag', 'student', 'seek', 'internship'], tags=['28']), TaggedDocument(words=['seek', 'human', 'resourc', 'opportun'], tags=['29']), TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['30']), TaggedDocument(words=['nativ', 'english', 'teacher', 'epik', 'english', 'program', 'korea'], tags=['31']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['32']), TaggedDocument(words=['peopl', 'develop', 'coordin', 'ryan'], tags=['33']), TaggedDocument(words=['advisori', 'board', 'member', 'celal', 'bayar', 'univers'], tags=['34']), TaggedDocument(words=['aspir', 'human', 'resourc', 'specialist'], tags=['35']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['36']), TaggedDocument(words=['hr', 'senior', 'specialist'], tags=['37']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['38']), TaggedDocument(words=['seek', 'human', 'resourc', 'hri', 'generalist', 'posit'], tags=['39']), TaggedDocument(words=['student', 'chapman', 'univers'], tags=['40']), TaggedDocument(words=['svp', 'chro', 'market', 'commun', 'csr', 'offic', 'engi', 'houston', 'woodland', 'energi', 'gphr', 'sphr'], tags=['41']), TaggedDocument(words=['human', 'resourc', 'coordin', 'intercontinent', 'buckhead', 'atlanta'], tags=['42']), TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['43']), TaggedDocument(words=['nativ', 'english', 'teacher', 'epik', 'english', 'program', 'korea'], tags=['44']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['45']), TaggedDocument(words=['peopl', 'develop', 'coordin', 'ryan'], tags=['46']), TaggedDocument(words=['advisori', 'board', 'member', 'celal', 'bayar', 'univers'], tags=['47']), TaggedDocument(words=['aspir', 'human', 'resourc', 'specialist'], tags=['48']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['49']), TaggedDocument(words=['hr', 'senior', 'specialist'], tags=['50']), TaggedDocument(words=['student', 'humber', 'colleg', 'aspir', 'human', 'resourc', 'generalist'], tags=['51']), TaggedDocument(words=['seek', 'human', 'resourc', 'hri', 'generalist', 'posit'], tags=['52']), TaggedDocument(words=['student', 'chapman', 'univers'], tags=['53']), TaggedDocument(words=['svp', 'chro', 'market', 'commun', 'csr', 'offic', 'engi', 'houston', 'woodland', 'energi', 'gphr', 'sphr'], tags=['54']), TaggedDocument(words=['human', 'resourc', 'coordin', 'intercontinent', 'buckhead', 'atlanta'], tags=['55']), TaggedDocument(words=['2019', 'bauer', 'colleg', 'busi', 'graduat', 'magna', 'cum', 'laud', 'aspir', 'human', 'resourc', 'profession'], tags=['56']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['57']), TaggedDocument(words=['peopl', 'develop', 'coordin', 'ryan'], tags=['58']), TaggedDocument(words=['aspir', 'human', 'resourc', 'specialist'], tags=['59']), TaggedDocument(words=['hr', 'senior', 'specialist'], tags=['60']), TaggedDocument(words=['seek', 'human', 'resourc', 'hri', 'generalist', 'posit'], tags=['61']), TaggedDocument(words=['student', 'chapman', 'univers'], tags=['62']), TaggedDocument(words=['svp', 'chro', 'market', 'commun', 'csr', 'offic', 'engi', 'houston', 'woodland', 'energi', 'gphr', 'sphr'], tags=['63']), TaggedDocument(words=['human', 'resourc', 'coordin', 'intercontinent', 'buckhead', 'atlanta'], tags=['64']), TaggedDocument(words=['experienc', 'retail', 'manag', 'aspir', 'human', 'resourc', 'profession'], tags=['65']), TaggedDocument(words=['human', 'resourc', 'staf', 'recruit', 'profession'], tags=['66']), TaggedDocument(words=['human', 'resourc', 'specialist', 'luxottica'], tags=['67']), TaggedDocument(words=['director', 'human', 'resourc', 'north', 'america', 'group', 'beneteau'], tags=['68']), TaggedDocument(words=['retir', 'armi', 'nation', 'guard', 'recruit', 'offic', 'manag', 'seek', 'posit', 'human', 'resourc'], tags=['69']), TaggedDocument(words=['human', 'resourc', 'generalist', 'scottmadden', 'inc'], tags=['70']), TaggedDocument(words=['busi', 'manag', 'major', 'aspir', 'human', 'resourc', 'manag'], tags=['71']), TaggedDocument(words=['aspir', 'human', 'resourc', 'manag', 'seek', 'internship', 'human', 'resourc'], tags=['72']), TaggedDocument(words=['human', 'resourc', 'profession'], tags=['73']), TaggedDocument(words=['nortia', 'staf', 'seek', 'human', 'resourc', 'payrol', 'administr', 'profession', '408'], tags=['74']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession', 'passion', 'help', 'creat', 'inclus', 'engag', 'work', 'environ'], tags=['75']), TaggedDocument(words=['human', 'conflict', 'polici', 'compens'], tags=['76']), TaggedDocument(words=['human', 'resourc', 'generalist', 'schwan'], tags=['77']), TaggedDocument(words=['liber', 'art', 'major', 'aspir', 'human', 'resourc', 'analyst'], tags=['78']), TaggedDocument(words=['junior', 'me', 'inform', 'system'], tags=['79']), TaggedDocument(words=['senior', 'human', 'resourc', 'busi', 'partner', 'heil', 'environment'], tags=['80']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession', 'energet', 'leader'], tags=['81']), TaggedDocument(words=['hr', 'manag', 'endemol', 'shine', 'north', 'america'], tags=['82']), TaggedDocument(words=['human', 'resourc', 'profession', 'world', 'leader', 'gi', 'softwar'], tags=['83']), TaggedDocument(words=['rrp', 'brand', 'portfolio', 'execut', 'jti', 'japan', 'tobacco', 'intern'], tags=['84']), TaggedDocument(words=['inform', 'system', 'specialist', 'programm', 'love', 'data', 'organ'], tags=['85']), TaggedDocument(words=['bachelor', 'scienc', 'biolog', 'victoria', 'univers', 'wellington'], tags=['86']), TaggedDocument(words=['human', 'resourc', 'manag', 'major'], tags=['87']), TaggedDocument(words=['director', 'human', 'resourc', 'ey'], tags=['88']), TaggedDocument(words=['undergradu', 'research', 'assist', 'styczynski', 'lab'], tags=['89']), TaggedDocument(words=['lead', 'offici', 'western', 'illinoi', 'univers'], tags=['90']), TaggedDocument(words=['seek', 'employ', 'opportun', 'within', 'custom', 'servic', 'patient', 'care'], tags=['91']), TaggedDocument(words=['admiss', 'repres', 'commun', 'medic', 'center', 'long', 'beach'], tags=['92']), TaggedDocument(words=['seek', 'human', 'resourc', 'opportun', 'open', 'travel', 'reloc'], tags=['93']), TaggedDocument(words=['student', 'westfield', 'state', 'univers'], tags=['94']), TaggedDocument(words=['student', 'indiana', 'univers', 'kokomo', 'busi', 'manag', 'retail', 'manag', 'delphi', 'hardwar', 'paint'], tags=['95']), TaggedDocument(words=['aspir', 'human', 'resourc', 'profession'], tags=['96']), TaggedDocument(words=['student'], tags=['97']), TaggedDocument(words=['seek', 'human', 'resourc', 'posit'], tags=['98']), TaggedDocument(words=['aspir', 'human', 'resourc', 'manag', 'graduat', 'may', '2020', 'seek', 'human', 'resourc', 'posit', 'loui'], tags=['99']), TaggedDocument(words=['human', 'resourc', 'generalist', 'loparex'], tags=['100']), TaggedDocument(words=['busi', 'intellig', 'analyt', 'travel'], tags=['101']), TaggedDocument(words=['alway', 'set', 'success'], tags=['102']), TaggedDocument(words=['director', 'administr', 'excel', 'log'], tags=['103'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Doc2Vec model\n",
        "model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=4, epochs=100)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n"
      ],
      "metadata": {
        "id": "F7v3q5R9WuHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding for \"Aspiring human resources\" (w1)\n",
        "w1 = 'Aspiring human resources'\n",
        "vector_w1 = model.infer_vector(preprocess_text(w1))"
      ],
      "metadata": {
        "id": "W7rwHsWDW4Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add another word \"seeking human resources\" (w2)\n",
        "w2 = 'Seeking human resources'\n",
        "vector_w2 = model.infer_vector(preprocess_text(w2))"
      ],
      "metadata": {
        "id": "zOF2kUepPsvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between w1 and w2\n",
        "similarity_w1_w2 = cosine_similarity([vector_w1], [vector_w2])[0][0]"
      ],
      "metadata": {
        "id": "Mct2HjPpnZgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the cosine similarity between w1 and w2\n",
        "print(f'Cosine Similarity between \"{w1}\" and \"{w2}\": {similarity_w1_w2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5EpvQ5LnOnE",
        "outputId": "283cf64e-7e70-4779-eb1d-e3efc0846e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between \"Aspiring human resources\" and \"Seeking human resources\": 0.9881563186645508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Calculate cosine similarity between w1, w2, and each job title\n",
        "\n",
        "similarities_w1 = []\n",
        "similarities_w2 = []\n",
        "for i, row in df.iterrows():\n",
        "    title = row['job_title']\n",
        "    vector_title = model.infer_vector(preprocess_text(title))\n",
        "    similarity_w1 = cosine_similarity([vector_w1], [vector_title])[0][0]\n",
        "    similarity_w2 = cosine_similarity([vector_w2], [vector_title])[0][0]\n",
        "    similarities_w1.append(similarity_w1)\n",
        "    similarities_w2.append(similarity_w2)"
      ],
      "metadata": {
        "id": "5LFvCimLZdDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cosine similarities to the DataFrame\n",
        "df['cosine_similarity_w1'] = similarities_w1\n",
        "df['cosine_similarity_w2'] = similarities_w2"
      ],
      "metadata": {
        "id": "WANsIOPcZjC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "q-n-2YWlbklT",
        "outputId": "fb1222dc-d733-4c9c-8db9-f4914feaa4a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'abc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-03cfd743661f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'abc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted sum of cosine_similarity and scaled_connection\n",
        "weight_cosine = 0.8\n",
        "weight_connection = 0.2\n",
        "df['ranking_w1'] = weight_cosine * df['cosine_similarity_w1'] + weight_connection * df['scaled_connection']\n",
        "df['ranking_w2'] = weight_cosine * df['cosine_similarity_w2'] + weight_connection * df['scaled_connection']"
      ],
      "metadata": {
        "id": "ETtLEcWsoohB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame based on the \"ranking_w1\" column in descending order\n",
        "df_sorted = df.sort_values(by='ranking_w1', ascending=False)"
      ],
      "metadata": {
        "id": "CXypwXkqotGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sorted DataFrame\n",
        "print(df_sorted[['job_title', 'scaled_connection', 'cosine_similarity_w1', 'cosine_similarity_w2', 'ranking_w1', 'ranking_w2']])"
      ],
      "metadata": {
        "id": "xaYsXJePRNIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e29ed1ad-ccab-48dd-9b7d-daf131140a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             job_title  scaled_connection  \\\n",
            "74   Nortia Staffing is seeking Human Resources, Pa...           1.000000   \n",
            "66   Human Resources, Staffing and Recruiting Profe...           1.000000   \n",
            "103   Director Of Administration at Excellence Logging           1.000000   \n",
            "77              Human Resources Generalist at Schwan's           1.000000   \n",
            "102                     Always set them up for Success           1.000000   \n",
            "..                                                 ...                ...   \n",
            "0    2019 C.T. Bauer College of Business Graduate (...           0.168337   \n",
            "30   2019 C.T. Bauer College of Business Graduate (...           0.168337   \n",
            "14   2019 C.T. Bauer College of Business Graduate (...           0.168337   \n",
            "18   2019 C.T. Bauer College of Business Graduate (...           0.168337   \n",
            "56   2019 C.T. Bauer College of Business Graduate (...           0.168337   \n",
            "\n",
            "     cosine_similarity_w1  cosine_similarity_w2  ranking_w1  ranking_w2  \n",
            "74               0.990397              0.994109    0.992318    0.995287  \n",
            "66               0.990100              0.990856    0.992080    0.992685  \n",
            "103              0.989776              0.994409    0.991821    0.995528  \n",
            "77               0.989595              0.993716    0.991676    0.994973  \n",
            "102              0.989414              0.993067    0.991532    0.994454  \n",
            "..                    ...                   ...         ...         ...  \n",
            "0                0.922188              0.923077    0.771418    0.772129  \n",
            "30               0.914202              0.916024    0.765029    0.766487  \n",
            "14               0.908060              0.909510    0.760115    0.761275  \n",
            "18               0.898424              0.901072    0.752407    0.754525  \n",
            "56               0.879133              0.881997    0.736974    0.739265  \n",
            "\n",
            "[104 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Bert Embeddings"
      ],
      "metadata": {
        "id": "lYRyEAgGkGDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "2p4R8HgibA1Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213aeae8-7b87-4723-cb27-89ad597a0ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "fIZ1JddHbJAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "k1REmqJDvDOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "yRN7vLUAbhqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d25d8b6-97c4-412c-b4b0-b368ea6698f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/potential-talents - Aspiring human resources - seeking human resources.csv\")"
      ],
      "metadata": {
        "id": "sj1fx0EFbkkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT model for sentence embeddings\n",
        "model = SentenceTransformer('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "EycWE_E-bn8O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c05045-41da-410c-cfcb-5b6473e1a066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-uncased. Creating a new one with MEAN pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calculating cosine similarity between two sentences\n",
        "def calculate_cosine_similarity(sentence1, sentence2):\n",
        "    embeddings = model.encode([sentence1, sentence2], convert_to_tensor=True)\n",
        "    similarity = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[1].reshape(1, -1))[0][0]\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "p25JZXQ9brt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get BERT embeddings for \"Aspiring human resources\" and \"Seeking human resources\"\n",
        "w1 = 'Aspiring human resources'\n",
        "w2 = 'Seeking human resources'\n",
        "bert_embedding_w1 = model.encode(w1, convert_to_tensor=True)\n",
        "bert_embedding_w2 = model.encode(w2, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "38NvvhqEb5Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity between w1, w2, and each job title\n",
        "similarities_w1 = []\n",
        "similarities_w2 = []\n",
        "for i, row in df.iterrows():\n",
        "    title = row['job_title']\n",
        "    bert_embedding_title = model.encode(title, convert_to_tensor=True)\n",
        "    similarity_w1 = cosine_similarity(bert_embedding_w1.reshape(1, -1), bert_embedding_title.reshape(1, -1))[0][0]\n",
        "    similarity_w2 = cosine_similarity(bert_embedding_w2.reshape(1, -1), bert_embedding_title.reshape(1, -1))[0][0]\n",
        "    similarities_w1.append(similarity_w1)\n",
        "    similarities_w2.append(similarity_w2)"
      ],
      "metadata": {
        "id": "RYsRERVkb8ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cosine similarities to the DataFrame\n",
        "df['cosine_similarity_bert_w1'] = similarities_w1\n",
        "df['cosine_similarity_bert_w2'] = similarities_w2"
      ],
      "metadata": {
        "id": "WNcOSJjyb_hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the DataFrame with job title, connection, and cosine similarities\n",
        "print(df[['job_title', 'connection', 'cosine_similarity_bert_w1', 'cosine_similarity_bert_w2']])"
      ],
      "metadata": {
        "id": "hd_U5TSvcCVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee021c0f-87a8-453f-8cc3-7e4b3035aa5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             job_title connection  \\\n",
            "0    2019 C.T. Bauer College of Business Graduate (...         85   \n",
            "1    Native English Teacher at EPIK (English Progra...      500+    \n",
            "2                Aspiring Human Resources Professional         44   \n",
            "3               People Development Coordinator at Ryan      500+    \n",
            "4      Advisory Board Member at Celal Bayar University      500+    \n",
            "..                                                 ...        ...   \n",
            "99   Aspiring Human Resources Manager | Graduating ...        103   \n",
            "100              Human Resources Generalist at Loparex      500+    \n",
            "101   Business Intelligence and Analytics at Travelers         49   \n",
            "102                     Always set them up for Success      500+    \n",
            "103   Director Of Administration at Excellence Logging      500+    \n",
            "\n",
            "     cosine_similarity_bert_w1  cosine_similarity_bert_w2  \n",
            "0                     0.700951                   0.642038  \n",
            "1                     0.621738                   0.585436  \n",
            "2                     0.947544                   0.864692  \n",
            "3                     0.832818                   0.789346  \n",
            "4                     0.566570                   0.566413  \n",
            "..                         ...                        ...  \n",
            "99                    0.737103                   0.676403  \n",
            "100                   0.774889                   0.741194  \n",
            "101                   0.781910                   0.732494  \n",
            "102                   0.711116                   0.707239  \n",
            "103                   0.801430                   0.743476  \n",
            "\n",
            "[104 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract numeric values from 'connection' column using regular expression\n",
        "df['connection'] = df['connection'].apply(lambda x: float(re.search(r'\\d+', x).group(0)))\n"
      ],
      "metadata": {
        "id": "IYwtjXZDcJ-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights for different components (adjust based on our preferences)\n",
        "weight_connection = 0.3\n",
        "weight_cosine_w1 = 0.5\n",
        "weight_cosine_w2 = 0.2"
      ],
      "metadata": {
        "id": "uPHS2sC9UAfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate fitness score\n",
        "df['fitness_score'] = (\n",
        "    weight_connection * df['connection'] +\n",
        "    weight_cosine_w1 * df['cosine_similarity_bert_w1'] +\n",
        "    weight_cosine_w2 * df['cosine_similarity_bert_w2']\n",
        ")"
      ],
      "metadata": {
        "id": "Q68d6UIDcBOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank candidates based on fitness score\n",
        "df['rank'] = df['fitness_score'].rank(ascending=False)"
      ],
      "metadata": {
        "id": "fg4FmMxHdX4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame based on fitness score in descending order\n",
        "df_sorted = df.sort_values(by='fitness_score', ascending=False)"
      ],
      "metadata": {
        "id": "rXXY2_9SlXV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sorted DataFrame with fitness score and rank\n",
        "print(df_sorted[['job_title', 'connection', 'cosine_similarity_bert_w1', 'cosine_similarity_bert_w2', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjQgDnz4dacO",
        "outputId": "6037a90b-fd97-48c8-ce69-496449e850d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              job_title  connection  \\\n",
            "25                 HR Senior Specialist       500.0   \n",
            "60                 HR Senior Specialist       500.0   \n",
            "37                 HR Senior Specialist       500.0   \n",
            "50                 HR Senior Specialist       500.0   \n",
            "7                  HR Senior Specialist       500.0   \n",
            "..                                  ...         ...   \n",
            "48  Aspiring Human Resources Specialist         1.0   \n",
            "35  Aspiring Human Resources Specialist         1.0   \n",
            "59  Aspiring Human Resources Specialist         1.0   \n",
            "5   Aspiring Human Resources Specialist         1.0   \n",
            "23  Aspiring Human Resources Specialist         1.0   \n",
            "\n",
            "    cosine_similarity_bert_w1  cosine_similarity_bert_w2  fitness_score   rank  \n",
            "25                   0.843815                   0.783556     150.578619    3.0  \n",
            "60                   0.843815                   0.783556     150.578619    3.0  \n",
            "37                   0.843815                   0.783556     150.578619    3.0  \n",
            "50                   0.843815                   0.783556     150.578619    3.0  \n",
            "7                    0.843815                   0.783556     150.578619    3.0  \n",
            "..                        ...                        ...            ...    ...  \n",
            "48                   0.927264                   0.841183       0.931868  102.0  \n",
            "35                   0.927264                   0.841183       0.931868  102.0  \n",
            "59                   0.927264                   0.841183       0.931868  102.0  \n",
            "5                    0.927264                   0.841183       0.931868  102.0  \n",
            "23                   0.927264                   0.841183       0.931868  102.0  \n",
            "\n",
            "[104 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose candidate with index 1 is starred, update its scores\n",
        "\n",
        "starred_candidate_index = 1\n",
        "df.at[starred_candidate_index, 'connection'] = 600  # Update connection score\n",
        "df.at[starred_candidate_index, 'cosine_similarity_bert_w1'] = 0.9  # Update cosine similarity score"
      ],
      "metadata": {
        "id": "fd232jwfuB6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculate fitness score\n",
        "df.at[starred_candidate_index, 'fitness_score'] = (\n",
        "    weight_connection * df.at[starred_candidate_index, 'connection'] +\n",
        "    weight_cosine_w1 * df.at[starred_candidate_index, 'cosine_similarity_bert_w1'] +\n",
        "    weight_cosine_w2 * df.at[starred_candidate_index, 'cosine_similarity_bert_w2']\n",
        ")"
      ],
      "metadata": {
        "id": "LH_W4anLuUVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recalculate rank\n",
        "df['rank'] = df['fitness_score'].rank(ascending=False)"
      ],
      "metadata": {
        "id": "46BknPutulNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-sort the DataFrame based on fitness score in descending order\n",
        "df_sorted = df.sort_values(by='fitness_score', ascending=False)\n"
      ],
      "metadata": {
        "id": "_YtE0h3CuoE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the updated DataFrame with new rankings\n",
        "print(\"\\nUpdated DataFrame:\")\n",
        "print(df_sorted[['job_title', 'connection', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Lnwq97u8xS",
        "outputId": "11124481-8d1d-43f0-e4c2-d41f0d9b091a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated DataFrame:\n",
            "                                            job_title  connection  \\\n",
            "1   Native English Teacher at EPIK (English Progra...       600.0   \n",
            "60                               HR Senior Specialist       500.0   \n",
            "37                               HR Senior Specialist       500.0   \n",
            "50                               HR Senior Specialist       500.0   \n",
            "7                                HR Senior Specialist       500.0   \n",
            "..                                                ...         ...   \n",
            "48                Aspiring Human Resources Specialist         1.0   \n",
            "35                Aspiring Human Resources Specialist         1.0   \n",
            "59                Aspiring Human Resources Specialist         1.0   \n",
            "5                 Aspiring Human Resources Specialist         1.0   \n",
            "23                Aspiring Human Resources Specialist         1.0   \n",
            "\n",
            "    fitness_score   rank  \n",
            "1      180.567087    1.0  \n",
            "60     150.578619    4.0  \n",
            "37     150.578619    4.0  \n",
            "50     150.578619    4.0  \n",
            "7      150.578619    4.0  \n",
            "..            ...    ...  \n",
            "48       0.931868  102.0  \n",
            "35       0.931868  102.0  \n",
            "59       0.931868  102.0  \n",
            "5        0.931868  102.0  \n",
            "23       0.931868  102.0  \n",
            "\n",
            "[104 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BONUS QUESTIONS"
      ],
      "metadata": {
        "id": "MUo-_GAVwsuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ranking gets better with each starring action."
      ],
      "metadata": {
        "id": "qVg-PLB-wv3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fitness_score(df, weight_connection, weight_cosine_w1, weight_cosine_w2):\n",
        "    # Calculate fitness score\n",
        "    df['fitness_score'] = (\n",
        "        weight_connection * df['connection'] +\n",
        "        weight_cosine_w1 * df['cosine_similarity_bert_w1'] +\n",
        "        weight_cosine_w2 * df['cosine_similarity_bert_w2']\n",
        "    )\n",
        "    return df"
      ],
      "metadata": {
        "id": "ghd-k4-Ew3xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_candidates(df):\n",
        "    # Rank candidates based on fitness score\n",
        "    df['rank'] = df['fitness_score'].rank(ascending=False)\n",
        "    return df"
      ],
      "metadata": {
        "id": "wVOe7npvw--g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def re_rank_starred_candidate(df, starred_candidate_index, updated_weights):\n",
        "    # Update the connection score and cosine similarity score for the starred candidate\n",
        "    df.at[starred_candidate_index, 'connection'] = 600  # Update connection score\n",
        "    df.at[starred_candidate_index, 'cosine_similarity_bert_w1'] = 0.9  # Update cosine similarity score\n",
        "\n",
        "    # Recalculate fitness score with updated weights\n",
        "    df = calculate_fitness_score(df, *updated_weights)\n",
        "\n",
        "    # Recalculate rank\n",
        "    df = rank_candidates(df)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "mci1Im1txDjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weights for different components (adjust based on your preferences)\n",
        "initial_weights = (0.3, 0.5, 0.2)\n",
        "updated_weights = (0.3, 0.4, 0.3)  # Adjusted weights after starring action"
      ],
      "metadata": {
        "id": "9uTKI29nywVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial ranking\n",
        "df = calculate_fitness_score(df, *initial_weights)\n",
        "df = rank_candidates(df)\n",
        "print(\"Initial Ranking:\")\n",
        "print(df[['job_title', 'connection', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5XK5b_Ly6v2",
        "outputId": "a525c49e-d1d0-4ce5-e583-537d475516df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Ranking:\n",
            "                                             job_title  connection  \\\n",
            "0    2019 C.T. Bauer College of Business Graduate (...        85.0   \n",
            "1    Native English Teacher at EPIK (English Progra...       600.0   \n",
            "2                Aspiring Human Resources Professional        44.0   \n",
            "3               People Development Coordinator at Ryan       500.0   \n",
            "4      Advisory Board Member at Celal Bayar University       500.0   \n",
            "..                                                 ...         ...   \n",
            "99   Aspiring Human Resources Manager | Graduating ...       103.0   \n",
            "100              Human Resources Generalist at Loparex       500.0   \n",
            "101   Business Intelligence and Analytics at Travelers        49.0   \n",
            "102                     Always set them up for Success       500.0   \n",
            "103   Director Of Administration at Excellence Logging       500.0   \n",
            "\n",
            "     fitness_score  rank  \n",
            "0        25.978883  59.0  \n",
            "1       180.567087   1.0  \n",
            "2        13.846711  81.5  \n",
            "3       150.574278  13.5  \n",
            "4       150.396567  42.5  \n",
            "..             ...   ...  \n",
            "99       31.403832  55.0  \n",
            "100     150.535683  23.0  \n",
            "101      15.237454  77.0  \n",
            "102     150.497006  28.0  \n",
            "103     150.549410  21.0  \n",
            "\n",
            "[104 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose candidate with index 1 is starred\n",
        "starred_candidate_index = 1\n",
        "df = re_rank_starred_candidate(df, starred_candidate_index, updated_weights)\n",
        "print(\"\\nAfter Starring Action:\")\n",
        "print(df[['job_title', 'connection', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b86NCYMzBUn",
        "outputId": "c53daa6f-b3eb-44a2-802f-f40dbf561e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Starring Action:\n",
            "                                             job_title  connection  \\\n",
            "0    2019 C.T. Bauer College of Business Graduate (...        85.0   \n",
            "1    Native English Teacher at EPIK (English Progra...       600.0   \n",
            "2                Aspiring Human Resources Professional        44.0   \n",
            "3               People Development Coordinator at Ryan       500.0   \n",
            "4      Advisory Board Member at Celal Bayar University       500.0   \n",
            "..                                                 ...         ...   \n",
            "99   Aspiring Human Resources Manager | Graduating ...       103.0   \n",
            "100              Human Resources Generalist at Loparex       500.0   \n",
            "101   Business Intelligence and Analytics at Travelers        49.0   \n",
            "102                     Always set them up for Success       500.0   \n",
            "103   Director Of Administration at Excellence Logging       500.0   \n",
            "\n",
            "     fitness_score  rank  \n",
            "0        25.972992  59.0  \n",
            "1       180.535631   1.0  \n",
            "2        13.838425  81.5  \n",
            "3       150.569931  13.5  \n",
            "4       150.396552  42.5  \n",
            "..             ...   ...  \n",
            "99       31.397762  55.0  \n",
            "100     150.532314  23.0  \n",
            "101      15.232512  77.0  \n",
            "102     150.496618  28.0  \n",
            "103     150.543615  21.0  \n",
            "\n",
            "[104 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df.sort_values(by='rank', ascending=True)\n",
        "print(df_sorted[['job_title', 'connection', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwbVUehIg0tF",
        "outputId": "fe1fa1f3-9887-441c-91e0-346a40a54e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            job_title  connection  \\\n",
            "1   Native English Teacher at EPIK (English Progra...       600.0   \n",
            "9   Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "39  Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "52  Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "61  Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "..                                                ...         ...   \n",
            "35                Aspiring Human Resources Specialist         1.0   \n",
            "48                Aspiring Human Resources Specialist         1.0   \n",
            "59                Aspiring Human Resources Specialist         1.0   \n",
            "23                Aspiring Human Resources Specialist         1.0   \n",
            "5                 Aspiring Human Resources Specialist         1.0   \n",
            "\n",
            "    fitness_score   rank  \n",
            "1      180.535631    1.0  \n",
            "9      150.574404    3.5  \n",
            "39     150.574404    3.5  \n",
            "52     150.574404    3.5  \n",
            "61     150.574404    3.5  \n",
            "..            ...    ...  \n",
            "35       0.923260  102.0  \n",
            "48       0.923260  102.0  \n",
            "59       0.923260  102.0  \n",
            "23       0.923260  102.0  \n",
            "5        0.923260  102.0  \n",
            "\n",
            "[104 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter out candidates which in the first place should not be in this list"
      ],
      "metadata": {
        "id": "g8g9uGbAd5bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclusion criteria: Exclude candidates with 'Excluded' in their job title\n",
        "exclusion_criteria = lambda x: x['job_title'].str.contains('Excluded', case=False)"
      ],
      "metadata": {
        "id": "SG2f2XNmOhkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_candidates(df, exclusion_criteria):\n",
        "    # Apply exclusion criteria to filter out candidates\n",
        "    filtered_df = df[~exclusion_criteria(df)]\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "24UHQcY3PYwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out candidates based on exclusion criteria\n",
        "filtered_df = filter_candidates(df, exclusion_criteria)\n",
        "print(\"\\nFiltered Ranking (Excluding Candidates):\")\n",
        "print(filtered_df[['job_title', 'connection', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FIn73YnOm52",
        "outputId": "47a554b5-78f9-45db-dab1-347f79a098c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtered Ranking (Excluding Candidates):\n",
            "                                             job_title  connection  \\\n",
            "0    2019 C.T. Bauer College of Business Graduate (...        85.0   \n",
            "1    Native English Teacher at EPIK (English Progra...       600.0   \n",
            "2                Aspiring Human Resources Professional        44.0   \n",
            "3               People Development Coordinator at Ryan       500.0   \n",
            "4      Advisory Board Member at Celal Bayar University       500.0   \n",
            "..                                                 ...         ...   \n",
            "99   Aspiring Human Resources Manager | Graduating ...       103.0   \n",
            "100              Human Resources Generalist at Loparex       500.0   \n",
            "101   Business Intelligence and Analytics at Travelers        49.0   \n",
            "102                     Always set them up for Success       500.0   \n",
            "103   Director Of Administration at Excellence Logging       500.0   \n",
            "\n",
            "     fitness_score  rank  \n",
            "0        25.972992  59.0  \n",
            "1       180.535631   1.0  \n",
            "2        13.838425  81.5  \n",
            "3       150.569931  13.5  \n",
            "4       150.396552  42.5  \n",
            "..             ...   ...  \n",
            "99       31.397762  55.0  \n",
            "100     150.532314  23.0  \n",
            "101      15.232512  77.0  \n",
            "102     150.496618  28.0  \n",
            "103     150.543615  21.0  \n",
            "\n",
            "[104 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To determine a cut-off point that would work for other roles without losing high potential candidates"
      ],
      "metadata": {
        "id": "u63F4GmeZW4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_cutoff(df, percentile_cutoff):\n",
        "    # Find the cutoff point based on the given percentile\n",
        "    cutoff_score = df['fitness_score'].quantile(percentile_cutoff)\n",
        "    return cutoff_score"
      ],
      "metadata": {
        "id": "H19sWETMZTk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_by_cutoff(df, cutoff_score):\n",
        "    # Filter candidates based on the cutoff score\n",
        "    filtered_df = df[df['fitness_score'] >= cutoff_score]\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "m6k0Ia6megmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the cutoff point based on a certain percentile (adjust as needed)\n",
        "percentile_cutoff = 0.75  # Example: Keep top 25% of candidates\n",
        "cutoff_score = find_cutoff(filtered_df, percentile_cutoff)\n",
        "print(f\"\\nCutoff Score: {cutoff_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPn2fTxIerXl",
        "outputId": "54b73425-ba1c-47af-f6da-2f70e97a9e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cutoff Score: 150.51058674752713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter candidates based on the cutoff score\n",
        "final_candidates = filter_by_cutoff(filtered_df, cutoff_score)\n",
        "print(\"\\nFinal Candidates:\")\n",
        "print(final_candidates[['job_title', 'connection', 'fitness_score', 'rank']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89kvAPtKe7R_",
        "outputId": "4c6e9e72-3f1d-4004-bbcd-8459cdd5ea7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Candidates:\n",
            "                                             job_title  connection  \\\n",
            "1    Native English Teacher at EPIK (English Progra...       600.0   \n",
            "3               People Development Coordinator at Ryan       500.0   \n",
            "7                                 HR Senior Specialist       500.0   \n",
            "9    Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "12   Human Resources Coordinator at InterContinenta...       500.0   \n",
            "17              People Development Coordinator at Ryan       500.0   \n",
            "21              People Development Coordinator at Ryan       500.0   \n",
            "25                                HR Senior Specialist       500.0   \n",
            "26   Aspiring Human Resources Management student se...       500.0   \n",
            "28   Aspiring Human Resources Management student se...       500.0   \n",
            "33              People Development Coordinator at Ryan       500.0   \n",
            "37                                HR Senior Specialist       500.0   \n",
            "39   Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "42   Human Resources Coordinator at InterContinenta...       500.0   \n",
            "46              People Development Coordinator at Ryan       500.0   \n",
            "50                                HR Senior Specialist       500.0   \n",
            "52   Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "55   Human Resources Coordinator at InterContinenta...       500.0   \n",
            "58              People Development Coordinator at Ryan       500.0   \n",
            "60                                HR Senior Specialist       500.0   \n",
            "61   Seeking Human Resources HRIS and Generalist Po...       500.0   \n",
            "64   Human Resources Coordinator at InterContinenta...       500.0   \n",
            "66   Human Resources, Staffing and Recruiting Profe...       500.0   \n",
            "67             Human Resources Specialist at Luxottica       500.0   \n",
            "70     Human Resources Generalist at ScottMadden, Inc.       500.0   \n",
            "100              Human Resources Generalist at Loparex       500.0   \n",
            "103   Director Of Administration at Excellence Logging       500.0   \n",
            "\n",
            "     fitness_score  rank  \n",
            "1       180.535631   1.0  \n",
            "3       150.569931  13.5  \n",
            "7       150.572593   8.0  \n",
            "9       150.574404   3.5  \n",
            "12      150.510587  25.5  \n",
            "17      150.569931  13.5  \n",
            "21      150.569931  13.5  \n",
            "25      150.572593   8.0  \n",
            "26      150.547417  18.5  \n",
            "28      150.547417  18.5  \n",
            "33      150.569931  13.5  \n",
            "37      150.572593   8.0  \n",
            "39      150.574404   3.5  \n",
            "42      150.510587  25.5  \n",
            "46      150.569931  13.5  \n",
            "50      150.572593   8.0  \n",
            "52      150.574404   3.5  \n",
            "55      150.510587  25.5  \n",
            "58      150.569931  13.5  \n",
            "60      150.572593   8.0  \n",
            "61      150.574404   3.5  \n",
            "64      150.510587  25.5  \n",
            "66      150.544577  20.0  \n",
            "67      150.549768  17.0  \n",
            "70      150.536854  22.0  \n",
            "100     150.532314  23.0  \n",
            "103     150.543615  21.0  \n"
          ]
        }
      ]
    }
  ]
}